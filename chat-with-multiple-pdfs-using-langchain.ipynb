{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain==0.1.1\n",
      "  Using cached langchain-0.1.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pypdf==4.0.0\n",
      "  Using cached pypdf-4.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting InstructorEmbedding==1.0.1\n",
      "  Using cached InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain==0.1.1) (5.4.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.1.1)\n",
      "  Using cached SQLAlchemy-2.0.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.1.1)\n",
      "  Using cached aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain==0.1.1)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.1)\n",
      "  Using cached dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.1)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.13 (from langchain==0.1.1)\n",
      "  Using cached langchain_community-0.0.16-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.9 (from langchain==0.1.1)\n",
      "  Using cached langchain_core-0.1.17-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain==0.1.1)\n",
      "  Using cached langsmith-0.0.85-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain==0.1.1) (1.26.3)\n",
      "Collecting pydantic<3,>=1 (from langchain==0.1.1)\n",
      "  Using cached pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain==0.1.1) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain==0.1.1) (8.2.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.1)\n",
      "  Using cached marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.1)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.1)\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting anyio<5,>=3 (from langchain-core<0.2,>=0.1.9->langchain==0.1.1)\n",
      "  Using cached anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.9->langchain==0.1.1) (23.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain==0.1.1)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.1 (from pydantic<3,>=1->langchain==0.1.1)\n",
      "  Using cached pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.1.1) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.1.1) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.1.1) (2020.6.20)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.1.1)\n",
      "  Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain==0.1.1)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain==0.1.1) (1.2.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.1)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached langchain-0.1.1-py3-none-any.whl (802 kB)\n",
      "Using cached pypdf-4.0.0-py3-none-any.whl (283 kB)\n",
      "Using cached InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
      "Using cached aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_community-0.0.16-py3-none-any.whl (1.6 MB)\n",
      "Using cached langchain_core-0.1.17-py3-none-any.whl (235 kB)\n",
      "Using cached langsmith-0.0.85-py3-none-any.whl (54 kB)\n",
      "Using cached pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
      "Using cached pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached SQLAlchemy-2.0.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Using cached marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: InstructorEmbedding, sniffio, pypdf, pydantic-core, mypy-extensions, multidict, marshmallow, jsonpointer, greenlet, frozenlist, async-timeout, annotated-types, yarl, typing-inspect, SQLAlchemy, pydantic, jsonpatch, anyio, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-community, langchain\n",
      "Successfully installed InstructorEmbedding-1.0.1 SQLAlchemy-2.0.25 aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.2.0 async-timeout-4.0.3 dataclasses-json-0.6.3 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.1 langchain-community-0.0.16 langchain-core-0.1.17 langsmith-0.0.85 marshmallow-3.20.2 multidict-6.0.4 mypy-extensions-1.0.0 pydantic-2.6.0 pydantic-core-2.16.1 pypdf-4.0.0 sniffio-1.3.0 typing-inspect-0.9.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain==0.1.1 pypdf==4.0.0 InstructorEmbedding==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME=\"collection_1\"\n",
    "DIM=512\n",
    "MILVUS_CONNECTION = {\"host\": \"127.0.0.1\", \"port\": \"19530\"}\n",
    "VECTOR_SEARCH_TOP_K = 6\n",
    "\n",
    "DocumentHome = '/home/ubuntu/sources/pdfs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!gdown 1v-Rn1FVU1pLTAQEgm0N9oB6cExMoebZr -O {DocumentHome}/tesla-earnings-report.pdf\n",
    "!gdown 1hm3dNy2DMX4Q_pF-bTfxILOeEYllX7Tb -O {DocumentHome}/docu88904_PowerMax-系列产品指南.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Documents to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('/home/ubuntu/sources/pdfs/docu88904_PowerMax-系列产品指南.pdf')   \n",
    "docs = loader.load()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader(DocumentHome)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: InstructorEmbedding==1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (1.0.1)\n",
      "Collecting sentence-transformers==2.2.2\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.37.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.2.0)\n",
      "Collecting torchvision (from sentence-transformers==2.2.2)\n",
      "  Using cached torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.26.3)\n",
      "Collecting scikit-learn (from sentence-transformers==2.2.2)\n",
      "  Using cached scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.12.0)\n",
      "Collecting nltk (from sentence-transformers==2.2.2)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.12.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->sentence-transformers==2.2.2) (8.0.3)\n",
      "Collecting joblib (from nltk->sentence-transformers==2.2.2)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence-transformers==2.2.2)\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
      "Using cached scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Using cached torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, nltk, torchvision, sentence-transformers\n",
      "Successfully installed joblib-1.3.2 nltk-3.8.1 scikit-learn-1.4.0 sentence-transformers-2.2.2 threadpoolctl-3.2.0 torchvision-0.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  InstructorEmbedding==1.0.1 sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"distiluse-base-multilingual-cased-v2\", model_kwargs={\"device\": DEVICE}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embedded data into Vector DB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pymilvus==2.3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymilvus\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from pymilvus import utility, connections, FieldSchema, CollectionSchema, DataType, Collection\n",
    "print(pymilvus.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Milvus server\n",
    "connections.connect(alias = \"default\", **MILVUS_CONNECTION)\n",
    "\n",
    "collectionList = utility.list_collections()\n",
    "\n",
    "if COLLECTION_NAME in collectionList:\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "collectionList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=DIM),\n",
    "]\n",
    "schema = CollectionSchema(fields=fields, description=\"Instructor Embeddings\")\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema) \n",
    "utility.has_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Milvus.from_documents(\n",
    "    texts,\n",
    "    embedding = embeddings,\n",
    "    collection_name = COLLECTION_NAME,\n",
    "    connection_args=MILVUS_CONNECTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is SRDF/Star?\"\n",
    "docs = vector_db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting chromadb==0.4.22\n",
      "  Using cached chromadb-0.4.22-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting build>=1.0.3 (from chromadb==0.4.22)\n",
      "  Using cached build-1.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: requests>=2.28 in /home/ubuntu/.local/lib/python3.10/site-packages (from chromadb==0.4.22) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from chromadb==0.4.22) (2.6.0)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.22)\n",
      "  Using cached chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb==0.4.22)\n",
      "  Using cached fastapi-0.109.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached uvicorn-0.27.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from chromadb==0.4.22) (1.26.3)\n",
      "Collecting posthog>=2.4.0 (from chromadb==0.4.22)\n",
      "  Using cached posthog-3.3.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from chromadb==0.4.22) (4.9.0)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.22)\n",
      "  Using cached pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb==0.4.22)\n",
      "  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb==0.4.22)\n",
      "  Using cached opentelemetry_api-1.22.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==0.4.22)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb==0.4.22)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb==0.4.22)\n",
      "  Using cached opentelemetry_sdk-1.22.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from chromadb==0.4.22) (0.15.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb==0.4.22)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from chromadb==0.4.22) (4.66.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb==0.4.22)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb==0.4.22)\n",
      "  Using cached importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb==0.4.22)\n",
      "  Using cached grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb==0.4.22)\n",
      "  Using cached bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb==0.4.22)\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb==0.4.22)\n",
      "  Using cached kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from chromadb==0.4.22) (8.2.3)\n",
      "Collecting PyYAML>=6.0.0 (from chromadb==0.4.22)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb==0.4.22)\n",
      "  Using cached mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=19.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from build>=1.0.3->chromadb==0.4.22) (23.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb==0.4.22)\n",
      "  Using cached pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting tomli>=1.1.0 (from build>=1.0.3->chromadb==0.4.22)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting starlette<0.36.0,>=0.35.0 (from fastapi>=0.95.2->chromadb==0.4.22)\n",
      "  Using cached starlette-0.35.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/lib/python3/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2.8.2)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached google_auth-2.27.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached websocket_client-1.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/lib/python3/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (1.26.5)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.22)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb==0.4.22)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: protobuf in /home/ubuntu/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.22) (4.25.2)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.22) (1.12)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb==0.4.22)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb==0.4.22)\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22)\n",
      "  Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_proto-1.22.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Using cached opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (69.0.3)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22)\n",
      "  Using cached asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.22)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic>=1.9->chromadb==0.4.22) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic>=1.9->chromadb==0.4.22) (2.16.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.22) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.28->chromadb==0.4.22) (3.3)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb==0.4.22) (0.20.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer>=0.9.0->chromadb==0.4.22) (8.0.3)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.4.22)\n",
      "  Using cached websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22) (5.3.2)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22) (2023.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.22) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb==0.4.22) (4.2.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.22)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.22) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb==0.4.22) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb==0.4.22) (1.2.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached chromadb-0.4.22-py3-none-any.whl (509 kB)\n",
      "Using cached chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "Using cached bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
      "Using cached build-1.0.3-py3-none-any.whl (18 kB)\n",
      "Using cached fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
      "Using cached grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Using cached kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Using cached mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
      "Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n",
      "Using cached opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
      "Using cached opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n",
      "Using cached opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached posthog-3.3.4-py2.py3-none-any.whl (40 kB)\n",
      "Using cached pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Using cached uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
      "Using cached importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
      "Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "Using cached httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached starlette-0.35.1-py3-none-any.whl (71 kB)\n",
      "Using cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "Using cached watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "Using cached websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: pypika, monotonic, mmh3, flatbuffers, wrapt, websockets, websocket-client, uvloop, typer, tomli, PyYAML, python-dotenv, pyasn1, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, oauthlib, importlib-resources, importlib-metadata, humanfriendly, httptools, h11, grpcio, googleapis-common-protos, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, rsa, requests-oauthlib, pyproject_hooks, pyasn1-modules, posthog, opentelemetry-exporter-otlp-proto-common, deprecated, coloredlogs, opentelemetry-api, onnxruntime, google-auth, fastapi, build, opentelemetry-sdk, opentelemetry-instrumentation, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "Successfully installed PyYAML-6.0.1 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 build-1.0.3 chroma-hnswlib-0.7.3 chromadb-0.4.22 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.109.0 flatbuffers-23.5.26 google-auth-2.27.0 googleapis-common-protos-1.62.0 grpcio-1.60.0 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-6.11.0 importlib-resources-6.1.1 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.17.0 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 overrides-7.7.0 posthog-3.3.4 pulsar-client-3.4.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 pypika-0.48.9 pyproject_hooks-1.0.0 python-dotenv-1.0.1 requests-oauthlib-1.3.1 rsa-4.9 starlette-0.35.1 tomli-2.0.1 typer-0.9.0 uvicorn-0.27.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websocket-client-1.7.0 websockets-12.0 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb==0.4.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load it into Chroma \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vector_db \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(documents\u001b[38;5;241m=\u001b[39m\u001b[43mtexts\u001b[49m, embedding\u001b[38;5;241m=\u001b[39membeddings, persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ubuntu/data/chroma/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# load it into Chroma \n",
    "vector_db = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=\"/home/ubuntu/data/chroma/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='SRDF/Star 解决方案 ..................................................................... 80第4章\\n第5章\\n第6章\\n第7章\\n第8章目录\\n4 产品指南   PowerMaxOS', metadata={'page': 3, 'source': '/home/ubuntu/sources/pdfs/docu88904_PowerMax-系列产品指南.pdf'}),\n",
       " Document(page_content='图 17 带有  R22 设备的并发  SRDF/Star\\nR11 R2 \\nR22SRDF/S\\nSRDF/A\\nSRDF/A\\nrecovery links Site B \\nActive\\nInactiveSite A \\nSite C \\n级联  SRDF/Star\\n在级联  SRDF/Star 解决方案中，同步辅助站点较之异步第三站点始终拥有更新的数\\n据。当同步辅助站点出现故障时，级联  SRDF/Star 解决方案可以在主站点和异步第三\\n站点之间以增量方式建立  SRDF/A 会话。\\n级联  SRDF/Star 可以确定当前活动  R1 周期（捕获）内容何时通过远距离  SRDF/A 链\\n路到达活动  R2 周期（应用）。这可将为实现完全同步而必须在站点  B 和站点  C 之间\\n移动的数据量降到最低。\\n此示例显示了一个基本的级联  SRDF/Star 解决方案。远程复制解决方案\\n82 产品指南   PowerMaxOS', metadata={'page': 81, 'source': '/home/ubuntu/sources/pdfs/docu88904_PowerMax-系列产品指南.pdf'}),\n",
       " Document(page_content='l与 TimeFinder 产品系列紧密集成\\nl地理位置分散的辅助站点和第三站点\\n当主站点出现故障时，级联  SRDF 只需极少的用户干预就可以从辅助站点到第三站点\\n继续镜像。这可以在第三站点实现更快的恢复。\\n辅助站点和第三站点都可以是故障切换站点。开放式系统解决方案通常故障切换到第\\n三站点。\\n级联  SRDF 可以与  SRDF/Star 一起实施。 级联  SRDF/Star （第  82 页）  介绍级联的\\nSRDF/Star 。\\n图 15 级联  SRDF 拓扑\\nSRDF/A or \\nAdaptive copy Site A\\nHostSite B Site C\\nR1 R2 SRDF/S, \\nSRDF/A or \\nAdaptive copy R21 \\nSRDF/Star 解决方案\\nSRDF/Star 是一种灾难恢复解决方案，它包含三个站点：主（生产）站点、辅助站点\\n和第三站点。辅助站点同步镜像来自主站点的数据，而第三站点则异步镜像生产数\\n据。\\n当主站点宕机时， SRDF/Star 解决方案让您可以快速在其余站点之间移动操作和重新\\n建立远程镜像。当条件允许时，您可以快速将主站点重新连接到解决方案，从而恢复\\nSRDF/Star 操作。\\nSRDF/Star 可以在并发和级联环境中运行，能够实现不同的恢复和可用性目标：\\nl并发  SRDF/Star － 数据从主站点并发镜像到两个  R2 设备。辅助站点和第三站点\\n都是潜在的恢复站点。辅助站点和第三站点之间使用差异再同步。\\nl级联  SRDF/Star － 数据首先从主站点镜像到辅助站点，然后再从辅助站点镜像到\\n第三站点。辅助站点和第三站点都是潜在的恢复站点。主站点和第三站点之间使用\\n差异再同步。\\n两个远程站点之间的差异同步能够：\\nl当主站点出现故障时，允许  SRDF/Star 快速重新建立跨站点镜像。\\nl大幅缩短远程镜像新生产站点所需的时间。\\n在发生影响主站点的雪崩式灾难时， SRDF/Star 可以帮助您确定哪个远程站点包含最\\n新数据。在从主站点故障恢复时，您可以选择要运行的站点以及要使用哪个站点的数\\n据。\\n当主站点出现故障时， SRDF/Star 让您只需进行极少的数据移动，就可以在辅助站点\\n和第三站点之间恢复异步保护。\\n用于开放式系统的  SRDF/Star', metadata={'page': 79, 'source': '/home/ubuntu/sources/pdfs/docu88904_PowerMax-系列产品指南.pdf'}),\n",
       " Document(page_content='表 13 SRDF 多站点解决方案   （续）\\n解决方案亮点 站点拓扑\\n“并发  SRDF ”\\n3 站点灾难恢复和高级多站点业务\\n连续性保护。\\nl主站点上的数据会同时复制到\\n2 个辅助站点。\\nl到远程站点的复制可以使用\\nSRDF/S、SRDF/A 或自适应拷\\n贝。\\n请参见： 并发  SRDF 解决方案 （第\\n78 页）。\\nSite A SRDF/S \\nadaptive copy \\nSite C Site B \\nR2 R11 R2 \\n“级联  SRDF ”\\n3 站点灾难恢复和高级多站点业务\\n连续性保护。\\nl主站点上的数据同步镜像到辅\\n助 (R21) 站点，然后从辅助\\n(R21) 站点异步镜像至第三\\n(R2) 站点。\\nl第一“跳”为  SRDF/S 。第二\\n跳是  SRDF/A 。\\n请参见： 级联  SRDF 解决方案 （第\\n79 页）。\\nSite A Site C Site B \\nSRDF/S SRDF/A \\nR1 R2 R21 \\n“SRDF/Star ”\\n提供  3 站点数据保护和灾难恢复，\\n以及零数据丢失恢复、业务连续性\\n保护和灾难重启。\\nl共有  2 种配置：\\nn级联  SRDF/Star\\nn并发  SRDF/Star\\nl在多站点灾难恢复实施中，差\\n异同步可以在幸存的站点中快\\n速重新建立镜像。\\nl使用带有  SRDF/S 和 SRDF/A\\n的 SRDF 一致性组  (CG) 实\\n施。\\n请参见： SRDF/Star 解决方案 （第\\n80 页）。\\nSite A SRDF/S SRDF/A \\nSRDF/A (recovery) R11\\nSite C Site B Cascaded SRDF/Star \\nConcurrent SRDF/Star \\nSite A SRDF/S \\nSRDF/A Site C Site B SRDF/A \\n(recovery) R11\\nR2/ \\nR22 R2/ \\nR22 R21 \\nR21 \\n并发  SRDF 解决方案\\n并发  SRDF 是一种  3 站点灾难恢复解决方案，该方案使用  R11 设备复制到两个  R2 设\\n备。两个  R2 设备使用任意组合的  SRDF 模式以并发方式独立工作：\\nl如果  R11 站点位于两个  R2 站点的同步距离内，则以并发  SRDF/S 模式复制到两个\\nR2 设备。远程复制解决方案\\n78 产品指南   PowerMaxOS', metadata={'page': 77, 'source': '/home/ubuntu/sources/pdfs/docu88904_PowerMax-系列产品指南.pdf'})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"什么是SRDF/Star?\"\n",
    "docs = vector_db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local vector database\n",
    "vector_db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_template_en = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "prompt_template_cn = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n",
    "如果无法从中得到答案，请说 \"根据已知信息无法回答该问题\" 或 \"没有提供足够的相关信息\"，不允许在答案中添加编造成分，答案请使用中文。\n",
    "已知内容:\n",
    "{context}\n",
    "问题:\n",
    "{question}\"\"\"\n",
    "\n",
    "prompt_template = prompt_template_cn\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Milvus DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Milvus(\n",
    "    connection_args = MILVUS_CONNECTION,\n",
    "    embedding_function = embeddings,\n",
    "    collection_name = COLLECTION_NAME,\n",
    "    drop_old = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(persist_directory=\"/home/ubuntu/data/chroma/\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM from API Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. llama2 with ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM Locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optimum in /home/ubuntu/.local/lib/python3.10/site-packages (1.16.2)\n",
      "Collecting auto-gptq\n",
      "  Downloading auto_gptq-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: coloredlogs in /home/ubuntu/.local/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers>=4.26.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (4.32.0)\n",
      "Requirement already satisfied: torch>=1.11 in /home/ubuntu/.local/lib/python3.10/site-packages (from optimum) (2.0.1)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (from optimum) (23.2)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from optimum) (1.26.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from optimum) (0.20.3)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.10/site-packages (from optimum) (2.13.0)\n",
      "Requirement already satisfied: accelerate>=0.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from auto-gptq) (0.26.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.10/site-packages (from auto-gptq) (0.1.99)\n",
      "Collecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Collecting gekko (from auto-gptq)\n",
      "  Downloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors in /home/ubuntu/.local/lib/python3.10/site-packages (from auto-gptq) (0.4.2)\n",
      "Requirement already satisfied: peft>=0.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from auto-gptq) (0.8.1)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from auto-gptq) (4.66.1)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate>=0.22.0->auto-gptq) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate>=0.22.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11->optimum) (69.0.3)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11->optimum) (0.42.0)\n",
      "Requirement already satisfied: cmake in /home/ubuntu/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11->optimum) (3.28.1)\n",
      "Requirement already satisfied: lit in /home/ubuntu/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11->optimum) (17.0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum) (0.13.3)\n",
      "Requirement already satisfied: protobuf in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (3.19.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->optimum) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->optimum) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->optimum) (2.2.0)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->optimum) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets->optimum) (3.9.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets->optimum) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\n",
      "Downloading auto_gptq-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rouge, gekko, auto-gptq\n",
      "Successfully installed auto-gptq-0.6.0 gekko-1.0.6 rouge-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optimum auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "model.safetensors: 100%|██████████| 7.26G/7.26G [02:01<00:00, 59.6MB/s]\n",
      "generation_config.json: 100%|██████████| 132/132 [00:00<00:00, 763kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    " \n",
    "MODEL_NAME = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
    ")\n",
    " \n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1024\n",
    "generation_config.temperature = 0.0001\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    " \n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    " \n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n_A._ I am the Spirit of the Land.\\n\\n_B._ What do you want with me?\\n\\n_A._ I have been watching you, and I see that you are a man of great potential. You have the power to make your dreams come true, but you lack the courage to take the first step. That is why I have come to offer my assistance. Together, we can achieve great things.\\n\\n_B._ How can you help me?\\n\\n_A._ I can guide you towards your goals, and provide you with the strength and courage you need to overcome any obstacles that stand in your way. But you must be willing to trust me and follow my guidance. Are you ready to take the first step on this journey?\\n\\n_B._ Yes, I'm ready. I trust you.\\n\\n_A._ Then let us begin. The journey ahead will not be easy, but with determination and hard work, you will reach your destination. Remember, I am always here to support and guide you. Trust in yourself and in me, and together we will achieve greatness.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ChatGLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 本地部署的 ChatGLM3-6B，自定义一个 LLM 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class ChatGLM_LLM(LLM):\n",
    "    # 基于本地 InternLM 自定义 LLM 类\n",
    "    tokenizer : AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "\n",
    "    def __init__(self, model_path :str):\n",
    "        # model_path: InternLM 模型路径\n",
    "        # 从本地初始化模型\n",
    "        super().__init__()\n",
    "        print(\"正在从本地加载模型...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(torch.bfloat16).cuda()\n",
    "        self.model = self.model.eval()\n",
    "        print(\"完成本地模型的加载\")\n",
    "\n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        # 重写调用函数\n",
    "        response, history = self.model.chat(self.tokenizer, prompt , history=[])\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ChatGLM3-6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成本地模型的加载\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的目标是针对用户的问题和要求提供适当的答复和支持。由于我是一个计算机程序，所以我没有实际的存在，只能通过互联网来与您交流。'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from LLM import ChatGLM_LLM\n",
    "llm = ChatGLM_LLM(model_path = \"/home/ubuntu/models/chatglm3-6b-32k\")\n",
    "llm.predict(\"你是谁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 百川LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (0.4.4)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.30.0-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting transformers_stream_generator\n",
      "  Using cached transformers_stream_generator-0.0.4-py3-none-any.whl\n",
      "Collecting cpm_kernels\n",
      "  Using cached cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
      "Collecting xformers\n",
      "  Using cached xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting numpy>=1.17 (from accelerate)\n",
      "  Using cached numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Collecting torch>=1.10.0 (from accelerate)\n",
      "  Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting huggingface-hub (from accelerate)\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate)\n",
      "  Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/lib/python3/dist-packages (from streamlit) (8.0.3)\n",
      "Requirement already satisfied: importlib-metadata<8,>=1.4 in /usr/lib/python3/dist-packages (from streamlit) (4.6.4)\n",
      "Collecting pandas<3,>=1.3.0 (from streamlit)\n",
      "  Using cached pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/lib/python3/dist-packages (from streamlit) (9.0.1)\n",
      "Collecting protobuf<5,>=3.20 (from streamlit)\n",
      "  Using cached protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting pyarrow>=6.0 (from streamlit)\n",
      "  Using cached pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from streamlit) (2.8.2)\n",
      "Collecting requests<3,>=2.27 (from streamlit)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Using cached rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting typing-extensions<5,>=4.3.0 (from streamlit)\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit)\n",
      "  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Using cached validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.41-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from streamlit) (6.4)\n",
      "Collecting watchdog>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "Collecting transformers>=4.26.1 (from transformers_stream_generator)\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "Collecting filelock (from torch>=1.10.0->accelerate)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sympy (from torch>=1.10.0->accelerate)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch>=1.10.0->accelerate)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.10.0->accelerate)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch>=1.10.0->accelerate)\n",
      "  Using cached fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==2.2.0 (from torch>=1.10.0->accelerate)\n",
      "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit)\n",
      "  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas<3,>=1.3.0->streamlit) (2022.1)\n",
      "Collecting tzdata>=2022.7 (from pandas<3,>=1.3.0->streamlit)\n",
      "  Using cached tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.27->streamlit) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.27->streamlit) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.27->streamlit) (2020.6.20)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.26.1->transformers_stream_generator)\n",
      "  Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.26.1->transformers_stream_generator)\n",
      "  Using cached tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers>=4.26.1->transformers_stream_generator)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.10.0->accelerate)\n",
      "  Using cached MarkupSafe-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.33.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached rpds_py-0.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.10.0->accelerate)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached streamlit-1.30.0-py2.py3-none-any.whl (8.4 MB)\n",
      "Using cached xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
      "Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "Using cached scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "Using cached altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "Using cached numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "Using cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Using cached tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Using cached validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Using cached tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Using cached referencing-0.33.0-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "Installing collected packages: sentencepiece, mpmath, cpm_kernels, watchdog, validators, tzlocal, tzdata, typing-extensions, tqdm, toolz, toml, tenacity, sympy, smmap, safetensors, rpds-py, regex, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mdurl, MarkupSafe, fsspec, filelock, charset-normalizer, cachetools, attrs, triton, scipy, requests, referencing, pyarrow, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, jinja2, gitdb, rich, pydeck, nvidia-cusolver-cu12, jsonschema-specifications, huggingface-hub, gitpython, bitsandbytes, torch, tokenizers, jsonschema, xformers, transformers, altair, accelerate, transformers_stream_generator, streamlit\n",
      "Successfully installed MarkupSafe-2.1.4 accelerate-0.26.1 altair-5.2.0 attrs-23.2.0 bitsandbytes-0.42.0 cachetools-5.3.2 charset-normalizer-3.3.2 cpm_kernels-1.0.11 filelock-3.13.1 fsspec-2023.12.2 gitdb-4.0.11 gitpython-3.1.41 huggingface-hub-0.20.3 jinja2-3.1.3 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pandas-2.2.0 protobuf-4.25.2 pyarrow-15.0.0 pydeck-0.8.1b0 referencing-0.33.0 regex-2023.12.25 requests-2.31.0 rich-13.7.0 rpds-py-0.17.1 safetensors-0.4.2 scipy-1.12.0 sentencepiece-0.1.99 smmap-5.0.1 streamlit-1.30.0 sympy-1.12 tenacity-8.2.3 tokenizers-0.15.1 toml-0.10.2 toolz-0.12.1 torch-2.2.0 tqdm-4.66.1 transformers-4.37.2 transformers_stream_generator-0.0.4 triton-2.2.0 typing-extensions-4.9.0 tzdata-2023.4 tzlocal-5.2 validators-0.22.0 watchdog-3.0.0 xformers-0.0.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate colorama bitsandbytes sentencepiece streamlit transformers_stream_generator cpm_kernels xformers scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n",
    "\n",
    "class Baichuan2_LLM(LLM):\n",
    "    # 基于本地 Baichuan 自定义 LLM 类\n",
    "    tokenizer : AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "\n",
    "    def __init__(self, model_path :str):\n",
    "        # model_path: Baichuan-7B-chat模型路径\n",
    "        # 从本地初始化模型\n",
    "        super().__init__()\n",
    "        print(\"正在从本地加载模型...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,torch_dtype=torch.bfloat16,  device_map=\"auto\")\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "        self.model = self.model.eval()\n",
    "        print(\"完成本地模型的加载\")\n",
    "\n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "         # 重写调用函数\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "         # 重写调用函数\n",
    "        response= self.model.chat(self.tokenizer, messages)\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"baichuan2_LLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成本地模型的加载\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我是百川大模型，是由百川智能的工程师们创造的大语言模型，我可以和人类进行自然交流、解答问题、协助创作，帮助大众轻松、普惠的获得世界知识和专业服务。如果你有任何问题，可以随时向我提问'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Baichuan2_LLM(model_path = \"/home/ubuntu/models/Baichuan2-13B-Chat\")\n",
    "llm.predict(\"你是谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 零一万物(01.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gradio>=4.13.0 protobuf>=4.25.1 torch==2.0.1 accelerate sentencepiece  datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-p8wxfr3m\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-p8wxfr3m\n",
      "  Resolved https://github.com/huggingface/transformers to commit 7b2bd1fbbd50e57cf28013e2d0737912ecc0f2eb\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.0.dev0)\n",
      "  Using cached tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers==4.38.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers==4.38.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers==4.38.0.dev0) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers==4.38.0.dev0) (2023.11.17)\n",
      "Using cached tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.38.0.dev0-py3-none-any.whl size=8451153 sha256=c05fd28e35fedeb9965482529c3500de5b80e65c7a4fcb7b79757e866c3592d8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1etendqz/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.32.0\n",
      "    Uninstalling transformers-4.32.0:\n",
      "      Successfully uninstalled transformers-4.32.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openllm 0.4.41 requires build[virtualenv]<1, but you have build 1.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.15.1 transformers-4.38.0.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig , LlamaTokenizerFast\n",
    "import torch\n",
    "\n",
    "class Yi_LLM(LLM):\n",
    "    # 基于本地 Yi 自定义 LLM 类\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "        \n",
    "    def __init__(self, model_path :str):\n",
    "\n",
    "        super().__init__()\n",
    "        print(\"正在从本地加载模型...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,torch_dtype=torch.bfloat16,device_map=\"auto\")\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "        self.model.generation_config.pad_token_id = self.model.generation_config.eos_token_id\n",
    "        self.model = self.model.eval()\n",
    "        print(\"完成本地模型的加载\")\n",
    "        \n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt }\n",
    "                    ]\n",
    "        input_ids = self.tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "    \n",
    "        output_ids = self.model.generate(input_ids.to('cuda'))\n",
    "        response = self.tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        return response\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Yi_LLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.45it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成本地模型的加载\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好！我是零一万物开发的智能助手，我叫 Yi，我是由工程师们通过大量的文本数据进行训练的。我拥有广泛的知识和能力，可以回答你的问题、提供信息，以及执行一些简单的任务。请问有什么我可以帮助你的？'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Yi_LLM(model_path = \"/home/ubuntu/models/Yi-6B-Chat\")\n",
    "llm.predict(\"你是谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 通义千问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==4.32.0\n",
      "  Using cached transformers-4.32.0-py3-none-any.whl.metadata (118 kB)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.local/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: tiktoken in /home/ubuntu/.local/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: einops in /home/ubuntu/.local/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.10/site-packages (1.12.0)\n",
      "Collecting transformers_stream_generator==0.0.4\n",
      "  Using cached transformers-stream-generator-0.0.4.tar.gz (12 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting peft\n",
      "  Using cached peft-0.8.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.32.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.32.0) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.32.0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.32.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.32.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.32.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.32.0) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.32.0)\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.32.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.32.0) (4.66.1)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers==4.32.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers==4.32.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers==4.32.0) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers==4.32.0) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (69.0.3)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.42.0)\n",
      "Requirement already satisfied: cmake in /home/ubuntu/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.28.1)\n",
      "Requirement already satisfied: lit in /home/ubuntu/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.8.1-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers_stream_generator\n",
      "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.4-py3-none-any.whl size=12316 sha256=405aae289708ad17d85082c38cef8004a02347e6ad136bfa763197ff998ebe15\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/47/1d/3c/92d88493ed40c0d9be60a391eb76c9a56e9f9b7542cb789401\n",
      "Successfully built transformers_stream_generator\n",
      "Installing collected packages: tokenizers, transformers, transformers_stream_generator, peft\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.1\n",
      "    Uninstalling tokenizers-0.15.1:\n",
      "      Successfully uninstalled tokenizers-0.15.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.0.dev0\n",
      "    Uninstalling transformers-4.38.0.dev0:\n",
      "      Successfully uninstalled transformers-4.38.0.dev0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openllm 0.4.41 requires build[virtualenv]<1, but you have build 1.0.3 which is incompatible.\n",
      "openllm 0.4.41 requires transformers[tokenizers,torch]>=4.36.0, but you have transformers 4.32.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed peft-0.8.1 tokenizers-0.13.3 transformers-4.32.0 transformers_stream_generator-0.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "\n",
    "class QwenLM(LLM):\n",
    "    # 基于本地 Qwen 自定义 LLM 类\n",
    "    tokenizer : AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "\n",
    "    def __init__(self, model_path :str):\n",
    "        # model_path: Qwen 模型路径\n",
    "        # 从本地初始化模型\n",
    "        super().__init__()\n",
    "        print(\"正在从本地加载模型...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "        # Specify hyperparameters for generation\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(model_path, trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n",
    "        print(\"完成本地模型的加载\")\n",
    "\n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        # 重写调用函数\n",
    "        response, history = self.model.chat(self.tokenizer, prompt , history=[])\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"QwenLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:07<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成本地模型的加载\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我是来自阿里云的大规模语言模型，我叫通义千问。'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = QwenLM(model_path = \"/home/ubuntu/models/Qwen-14B-Chat\")\n",
    "llm.predict(\"你是谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"THUDM/chatglm-6b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\", \n",
    "    model=model_name, \n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建检索问答链, 然后与文档对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是百川大模型，是由百川智能的工程师们创造的大语言模型，我可以和人类进行自然交流、解答问题、协助创作，帮助大众轻松、普惠的获得世界知识和专业服务。如果你有任何问题，可以随时向我提问'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"你是谁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#retriever = vector_store.as_retriever(search_kwargs={\"k\": VECTOR_SEARCH_TOP_K})\n",
    "retriever = vector_store.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': '什么是SRDF/Star?',\n",
       " 'result': 'SRDF/Star是一种灾难恢复解决方案，由三个站点组成：主（生产）站点、辅助站点和第三站点。辅助站点同步镜像来自主站点的数据，而第三站点则异步镜像生产数据。当主站点出现故障时，SRDF/Star解决方案允许您在其余站点之间快速移动操作并在远程站点重建镜像。当主站点出现故障时，SR',\n",
       " 'source_documents': [Document(page_content='SRDF/Star 解决方案 ..................................................................... 80第4章\\n第5章\\n第6章\\n第7章\\n第8章目录\\n4 产品指南   PowerMaxOS', metadata={'page': 3, 'source': '/home/ubuntu/sources/pdfs/docu88904_PowerMax-系列产品指南.pdf'}),\n",
       "  Document(page_content='图 17 带有  R22 设备的并发  SRDF/Star\\nR11 R2 \\nR22SRDF/S\\nSRDF/A\\nSRDF/A\\nrecovery links Site B \\nActive\\nInactiveSite A \\nSite C \\n级联  SRDF/Star\\n在级联  SRDF/Star 解决方案中，同步辅助站点较之异步第三站点始终拥有更新的数\\n据。当同步辅助站点出现故障时，级联  SRDF/Star 解决方案可以在主站点和异步第三\\n站点之间以增量方式建立  SRDF/A 会话。\\n级联  SRDF/Star 可以确定当前活动  R1 周期（捕获）内容何时通过远距离  SRDF/A 链\\n路到达活动  R2 周期（应用）。这可将为实现完全同步而必须在站点  B 和站点  C 之间\\n移动的数据量降到最低。\\n此示例显示了一个基本的级联  SRDF/Star 解决方案。远程复制解决方案\\n82 产品指南   PowerMaxOS', metadata={'page': 81, 'source': '/home/ubuntu/sources/pdfs/docu88904_PowerMax-系列产品指南.pdf'}),\n",
       "  Document(page_content='l与 TimeFinder 产品系列紧密集成\\nl地理位置分散的辅助站点和第三站点\\n当主站点出现故障时，级联  SRDF 只需极少的用户干预就可以从辅助站点到第三站点\\n继续镜像。这可以在第三站点实现更快的恢复。\\n辅助站点和第三站点都可以是故障切换站点。开放式系统解决方案通常故障切换到第\\n三站点。\\n级联  SRDF 可以与  SRDF/Star 一起实施。 级联  SRDF/Star （第  82 页）  介绍级联的\\nSRDF/Star 。\\n图 15 级联  SRDF 拓扑\\nSRDF/A or \\nAdaptive copy Site A\\nHostSite B Site C\\nR1 R2 SRDF/S, \\nSRDF/A or \\nAdaptive copy R21 \\nSRDF/Star 解决方案\\nSRDF/Star 是一种灾难恢复解决方案，它包含三个站点：主（生产）站点、辅助站点\\n和第三站点。辅助站点同步镜像来自主站点的数据，而第三站点则异步镜像生产数\\n据。\\n当主站点宕机时， SRDF/Star 解决方案让您可以快速在其余站点之间移动操作和重新\\n建立远程镜像。当条件允许时，您可以快速将主站点重新连接到解决方案，从而恢复\\nSRDF/Star 操作。\\nSRDF/Star 可以在并发和级联环境中运行，能够实现不同的恢复和可用性目标：\\nl并发  SRDF/Star － 数据从主站点并发镜像到两个  R2 设备。辅助站点和第三站点\\n都是潜在的恢复站点。辅助站点和第三站点之间使用差异再同步。\\nl级联  SRDF/Star － 数据首先从主站点镜像到辅助站点，然后再从辅助站点镜像到\\n第三站点。辅助站点和第三站点都是潜在的恢复站点。主站点和第三站点之间使用\\n差异再同步。\\n两个远程站点之间的差异同步能够：\\nl当主站点出现故障时，允许  SRDF/Star 快速重新建立跨站点镜像。\\nl大幅缩短远程镜像新生产站点所需的时间。\\n在发生影响主站点的雪崩式灾难时， SRDF/Star 可以帮助您确定哪个远程站点包含最\\n新数据。在从主站点故障恢复时，您可以选择要运行的站点以及要使用哪个站点的数\\n据。\\n当主站点出现故障时， SRDF/Star 让您只需进行极少的数据移动，就可以在辅助站点\\n和第三站点之间恢复异步保护。\\n用于开放式系统的  SRDF/Star', metadata={'page': 79, 'source': '/home/ubuntu/sources/pdfs/docu88904_PowerMax-系列产品指南.pdf'}),\n",
       "  Document(page_content='表 13 SRDF 多站点解决方案   （续）\\n解决方案亮点 站点拓扑\\n“并发  SRDF ”\\n3 站点灾难恢复和高级多站点业务\\n连续性保护。\\nl主站点上的数据会同时复制到\\n2 个辅助站点。\\nl到远程站点的复制可以使用\\nSRDF/S、SRDF/A 或自适应拷\\n贝。\\n请参见： 并发  SRDF 解决方案 （第\\n78 页）。\\nSite A SRDF/S \\nadaptive copy \\nSite C Site B \\nR2 R11 R2 \\n“级联  SRDF ”\\n3 站点灾难恢复和高级多站点业务\\n连续性保护。\\nl主站点上的数据同步镜像到辅\\n助 (R21) 站点，然后从辅助\\n(R21) 站点异步镜像至第三\\n(R2) 站点。\\nl第一“跳”为  SRDF/S 。第二\\n跳是  SRDF/A 。\\n请参见： 级联  SRDF 解决方案 （第\\n79 页）。\\nSite A Site C Site B \\nSRDF/S SRDF/A \\nR1 R2 R21 \\n“SRDF/Star ”\\n提供  3 站点数据保护和灾难恢复，\\n以及零数据丢失恢复、业务连续性\\n保护和灾难重启。\\nl共有  2 种配置：\\nn级联  SRDF/Star\\nn并发  SRDF/Star\\nl在多站点灾难恢复实施中，差\\n异同步可以在幸存的站点中快\\n速重新建立镜像。\\nl使用带有  SRDF/S 和 SRDF/A\\n的 SRDF 一致性组  (CG) 实\\n施。\\n请参见： SRDF/Star 解决方案 （第\\n80 页）。\\nSite A SRDF/S SRDF/A \\nSRDF/A (recovery) R11\\nSite C Site B Cascaded SRDF/Star \\nConcurrent SRDF/Star \\nSite A SRDF/S \\nSRDF/A Site C Site B SRDF/A \\n(recovery) R11\\nR2/ \\nR22 R2/ \\nR22 R21 \\nR21 \\n并发  SRDF 解决方案\\n并发  SRDF 是一种  3 站点灾难恢复解决方案，该方案使用  R11 设备复制到两个  R2 设\\n备。两个  R2 设备使用任意组合的  SRDF 模式以并发方式独立工作：\\nl如果  R11 站点位于两个  R2 站点的同步距离内，则以并发  SRDF/S 模式复制到两个\\nR2 设备。远程复制解决方案\\n78 产品指南   PowerMaxOS', metadata={'page': 77, 'source': '/home/ubuntu/sources/pdfs/docu88904_PowerMax-系列产品指南.pdf'})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"什么是SRDF/Star?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
